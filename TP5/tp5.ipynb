{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Preprocesamiento de datos\n",
    "\n",
    "##### 1.1.1. \n",
    "A la hora de preparar los datos antes de ser entrenados existen tres acciones básicas a realizar, explique en que consiste cada una de las siguientes y como se implementarían en Python: Mean subtraction, Normalization, Scaling.\n",
    "\n",
    "Aplicar estas tres funciones sobre un dataset a elegir de scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Mean Substraction o Mean Removal \n",
    "#### [Teoría]: \n",
    " \n",
    "La intención de quitar la media al momento de preparar diferentes datos para ser utilizados a futuro, es lograr de algún modo una estandarización de dichos datos para que puedan ser comparados entre sí.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "Si pensamos en la medición de temperaturas, los valores numéricos de los promedios de temperatura dependen de que escala se utilice (Fahrenheit, Celsius o Kelvin). La elección de un punto central (es decir, un cero) para nuestras mediciones, estará ligada a la escala usada.\n",
    "\n",
    "Pero, si eliminamos la media, quitamos la influencia de dicha elección. Aún así, la unidad de medida sigue estando visible de algún modo en los datos, puesto que la nocion de \"1 grado de temperatura\" es diferente en las diferentes escalas. La division por σ remueve las unidades y obtenemos una cantidad sin unidad (\"z-score\") que es independiente de la escala de temperatura utilizada. (Teniendo en cuenta que la escala es linear y que el sentido de \"más caliente\" representa una \"mayor temperatura\").\n",
    "\n",
    "A gran escala, la idea es permitir que diferentes conjuntos de datos sean comparables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Aplicación]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graficar(x, y):\n",
    "    # get the product class \n",
    "    product_class = np.unique(Y)\n",
    "    colors = plt.get_cmap(\"hsv\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i, p in enumerate(product_class):\n",
    "        mask = (y == p)\n",
    "        print mask.shape\n",
    "        plt.scatter(x[mask, 0], x[mask, 1], \n",
    "                    c=colors(1. * i / 11), label=p, alpha=0.2)\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "print \"Datos: \\n\", X[:10]\n",
    "print \"...\"\n",
    "graficar(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "mean_ = scaler.mean_\n",
    "std_ = scaler.std_\n",
    "print \"Mean: %s \\nStd: %s\" % (mean_, std_)\n",
    "print \"Data w/ mean removal applied (total: %d):\\n %s\" % (len(X_scaled), X_scaled[:10])\n",
    "print \"...\"\n",
    "graficar(X_scaled, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Normalización:\n",
    "#### [Teoría]:\n",
    "\n",
    "En estadística y aplicaciones de estadística, la normalización puede tener varios significados. En los casos más simples, la normalización se refiera a el ajuste de valores medidos en diferentes escalas a una escala común, a menudo antes de trabajar con promedios.\n",
    "\n",
    "En casos más complicados, la normalización puede referirse a ajustes más sofisticados donde la intención es \"alinear\" las diferentes distribuciones de probabilidad que poseen los datos.\n",
    "\n",
    "Un enfoque distinto es la normalización cuantil, donde los cuantiles de diferentes mediciones, son llevados a una forma común.\n",
    "\n",
    "En otro uso estadístico, el proceso de normalización se refiere a la creación de versiones escaladas y cambiadas de las estadísticas, donde la intención es que dichos valores normalizados permitan su comparación con otros valores (también normalizados), pertenecientes a otros conjuntos de datos, de modo que pueda ser posible eliminar ciertas influencias externas (por ejemplo: unidades de medida)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Scaling:\n",
    "#### [Teoría]:\n",
    "\n",
    "\n",
    "Típicamente, en lo referente a estadística, la acción de escalar datos tiene que ver con transformar linealmente a los valores para poder trabajar con ellos. Por ejemplo: llevando todos los valores del conjunto de datos a una escala propiamente dicha, cuyos valores esten comprendidos entre 0 y 1, o entre -1 y 1.\n",
    "\n",
    "La normalización, en cambio, puede involucrar tanto el hecho de aplicar una transformación para que los datos estén distribuidos normalmente, o bien poner los valores en una escala común."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Cuando trabajamos con datos de muchas dimensiones, suele ser necesario reducir esta dimensionalidad sin perder mucha información de nuestros datos, una opción para realizar esto es aplicar PCA (principal component analysis) sobre los mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Aplique PCA sobre el dataset otto(ver ipynb con links a dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Test dataset\n",
    "test = read_csv('data/test.csv')\n",
    "\n",
    "# Train dataset\n",
    "train = read_csv('data/train.csv')\n",
    "X = train.ix[:,1:-1].values\n",
    "#print X\n",
    "y = train.ix[:, -1]\n",
    "#print y\n",
    "\n",
    "types = np.sort(np.unique(y))\n",
    "new_values = dict(zip(types, range(types.shape[0])))\n",
    "y = (y.map(new_values).astype(np.int32)).values\n",
    "print y\n",
    "target_names = np.unique(y)\n",
    "\n",
    "# con 18 componentes tenemos 0.6979% explicada la variablidad\n",
    "# con 27 componentes tenemos 0.8006% explicada la variablidad\n",
    "def get_n_comp_expl_var(variance):\n",
    "    for i in range(100):\n",
    "        pca = PCA(n_components=i)\n",
    "        pca.fit(X)\n",
    "        suma = pca.explained_variance_ratio_.sum()\n",
    "        if suma >= variance:\n",
    "            return i, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Cuantos componentes se deben usar para explicar la variabilidad del 70 % y 80 % de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variances = [0.7, 0.8]\n",
    "pcas = []\n",
    "for v in variances:\n",
    "    res = get_n_comp_expl_var(v)\n",
    "    n_comp = res[0]\n",
    "    pca = res[1]\n",
    "    pcas.append(pca)\n",
    "    print \"Para explicar el %d %% de variabilidad de los datos,\\\n",
    "            son necesarios %d componentes\" % (v*100, n_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Grafique los primeros dos PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def graficar_pca(x_transformed, y, target_names):\n",
    "    colors = plt.get_cmap(\"hsv\")\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i, p in enumerate(target_names):\n",
    "        mask = (y == p)\n",
    "        plt.scatter(x_transformed[mask, 0], x_transformed[mask, 1], \n",
    "                c=colors(1. * i / 11), label=p, alpha=0.2)\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "for pca in pcas:\n",
    "    x_transformed = pca.fit_transform(X)\n",
    "    graficar_pca(x_transformed, y, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.2. Introducción a aprendizaje automático\n",
    "Dentro del aprendizaje automático, contamos con varias áreas de aplicación, entre ellas podemos encontrar: clasificación, regresiones, clustering y reducción de dimensiones. \n",
    "\n",
    "Explique que problemas resuelven los métodos que se encuentran dentro de estas áreas, explique un método dentro de cada una de ellas y de una posible aplicación.\n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "\n",
    "Los métodos que se encuentran dentro de esas áreas resuelven los siguientes problemas:\n",
    "- Clasificación: Identificar a cuál categoría pertenece un objeto.\n",
    "- Regresión: Predecir un valor contínuo asociado a un objeto.\n",
    "- Clustering: Agrupación automática de objetos similares en conjuntos.\n",
    "- Reducción de dimensionalidad: Reducir el número de variables al azar a considerar.\n",
    "\n",
    "#### Métodos para clasificación y regresión:\n",
    "- Modelos lineales generalizados (regresión: bayesiana, logística, polinomial, entre otros).\n",
    "- Modelos lineales y cuadráticos para análisis discriminante.\n",
    "- Máquinas de vectores de soporte. (clasificación, regresión, formulación matemática, estimación de densidad, complejidad, entre otros).\n",
    "- Stochastic Gradient Descent (Clasificación, regresión, complejidad, entre otros).\n",
    "- Vecinos más cercanos (Clasificación, regresión, entre otros).\n",
    "- Procesos gausseanos (Formulacion matemática, modelos de correlación, modelos de regresión).\n",
    "- Descomposicion cruzada\n",
    "- Naive Bayes (Gauss Naive Bayes, Multinomial Naive Bayes, Bernoulli Naive Bayes, entre otros).\n",
    "- Arboles de decisión (Clasificación, regresión, problemas con múltiples resultados, complejidad, entre otros).\n",
    "\n",
    "Aplicación: \n",
    "- Clasificacion: El uso de SVM's para determinar a que número del 0 al 9, corresponde un digito manuscrito y proporcinado como entrada.\n",
    "- Regresión: Predicción del valor de una casa en función de su superficie útil, número de habitaciones, cuartos de baños, etc\n",
    "\n",
    "#### Métodos para clustering:\n",
    "- K-means\n",
    "- Affinity propagation\n",
    "- Mean-shift\n",
    "- Spectral clustering\n",
    "- Agglomerative clustering\n",
    "- Entre otros\n",
    "\n",
    "Aplicación: \n",
    "Las técnicas de agrupamiento encuentran aplicación en diversos ámbitos.\n",
    "- En biología para clasificar animales y plantas.\n",
    "- En medicina para identificar enfermedades.\n",
    "- En marketing para identificar personas con hábitos de compras similares.\n",
    "- En teoría de la señal pueden servir para eliminar ruidos.\n",
    "- En biometría para identificación del locutor o de caras.\n",
    "\n",
    "#### Métodos para reducción de dimensionalidad:\n",
    "- Exact PCA and probabilistic interpretation\n",
    "- Incremental PCA\n",
    "- Approximate PCA\n",
    "- Kernel PCA\n",
    "- SparcePCA\n",
    "- MiniBatchSparcePCA\n",
    "- Dictionary Learning\n",
    "- Factor Analysis\n",
    "- Independent Component Analysis (ICA)\n",
    "- Entre otros\n",
    "\n",
    "Aplicación:\n",
    "\n",
    "Una análisis de 11 indicadores socieconómicos de 96 países, reveló que los resultados podían explicarse en alto grado a partir de sólo dos componentes principales, el primero de ellos tenía que ver con el nivel de PIB total del país y el segundo con el índice de ruralidad.\n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "\n",
    "¿La cantidad de datos disponibles para entrenamiento pueden condicionar la elección de algún método sobre otro? ¿Y el tipo de dato? \n",
    "\n",
    "Primero que nada, cabe aclarar que la elección del método depende principalmente del problema. Si se dispone de un conjunto de datos significativo y si se trata de un problema que se pueda resolver con aprendizaje supervisado o bien con aprendizaje no supervisado. \n",
    "\n",
    "Algunos casos particulares: \n",
    "\n",
    "**Mandarin Voice Conversion Using Tone Codebook Mapping:** \n",
    "\n",
    "Utiliza una versión adaptada de un algoritmo de clustering llamada Linde–Buzo–Gray o LBG, similar a k-means, pero específico para trabajar con señales de audio (voz) y  debido a su efectividad para realizar clustering sobre los vectores de tono (pitch) de dichas señales y aprender los codebooks (conjunto de muestras de tonos) de los distintos tonos del dataset.\n",
    "\n",
    "Este algoritmo crea una cantidad de clusters igual a una potencia de 2 para poder trabajar. A su vez, se sabe que en cuanto a performance el algoritmo se comporta de manera similar al algoritmo de Lloyd, en términos de precision y velocidad en lo que se refiere al clustering de features en problemas de reconocimiento del habla, siendo que el algoritmo de Lloyd funciona a buena velocidad con grandes conjuntos de datos y también presenta una ventaja en cuanto a tiempo de procesamiento y en términos de error cuadrático medio comparado con otros algoritmos [1].\n",
    "\n",
    "[1]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2423442/\n",
    "\n",
    "**Location of Tropical Cyclone Center with Intelligent Image Processing Technique**\n",
    "\n",
    "La idea básica detrás de este experimento, es encontrar los centros de los ciclones tropicales basados en la utilización de técnicas para procesamiento inteligente de imagen además de otras caracteristicas como temperatura y trayectoria de los mismos.\n",
    "\n",
    "Dentro de las técnicas utilizadas, se mencionan: *Noise Reduction* e *Image Segmentation* para disminuir el ruido de las imagenes satelitales y acotar la imagen a procesar al área adecuada que contenga los datos y de ese modo reducir la cantidad de datos a procesar, así tambien como *Rotation Center Method* para simular el comportamiento rotativo de los ciclones tropicales.\n",
    "\n",
    "**Research and Application of Data Mining in Power Plant Process Control and Optimization **\n",
    "\n",
    "Como su nombre en inglés lo indica, utiliza técnicas de *Data Mining* para el control y optimizacion de las operaciones dentro de una planta de energía debido al volúmen de datos que estas reciben en tiempo real y en función de los factores más relevantes para dichas operaciones.\n",
    "\n",
    "Se puede extraer mucho conocimiento existente en los datos historicos pero es dificil de encontrar y resumirlos del modo tradicional debido al gran volúmen y al fuerte acoplamiento que los datos tienen en el proceso de la electricidad industrial\n",
    "\n",
    "Las técnicas de data mining proveen un modo efectivo y moderno de resolver estas dificultades. Puede adquirir conocimiento útil y e inferir reglas o tendencias hallados en los datos almacenados y proveer un mejor soporte para la toma de decisiones y una forma de optimización de los procesos industriales, por lo que resulta importante encontrar dicha informacion en los datos históricos para mejorar los sistemas de energía.\n",
    "\n",
    "Dentro de las técnicas utilizadas, se menciona: *Fuzzy Association Rule Mining* que sirve para descubrir importantes relaciones entre diferentes items. Aunque se utiliza una versión mejorada del algoritmo para encontrar los valores necesarios para optimizar los procesos de la planta de energía.\n",
    "\n",
    "A su vez, La determinación de los valores de optimización basados en minería de datos tiene varias virtudes por sobre los métodos tradicionales: \n",
    "\n",
    "* El valor puede ser regulado dinámicamente de acuerdo a las diferentes necesidades tales como el bajo consumo de carbón o bajo nivel de emisiones de contaminación. \n",
    "\n",
    "* El valor que se determina a partir de datos históricos es acorde a la realidad y razonable en la práctica.\n",
    "\n",
    "* El valor de optimización se presenta en un intervalo, por lo que el operador tiene más opciones la hora de optimizar el proceso industrial.\n",
    "\n",
    "\n",
    "En el inciso [1.3.1] del presente documento, pueden apreciarse entre otros factores, la escalabilidad en cuanto a 3 métodos de clustering: K-Means, Mean-shift y Gaussian mixtures. Es decir, cómo éstos se comportan bajo una alta exigencia en cuanto a cantidad de clusters presentes y a cantidad de muestras disponibles.\n",
    "\n",
    "[1.3.1]:http://localhost:8888/notebooks/tp5.ipynb#1.3.-Clustering\n",
    "\n",
    "**Conclusión:** \n",
    "\n",
    "Puede asegurarse que tanto el tipo de datos, como la cantidad de los mismos así tambien como la naturaleza del problema condicionan la elección de los diferentes métodos.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "En la implementación, ¿Cuál es el pipe utilizado para analizar datos con una SVM? Explique parámetros y función de cada paso.\n",
    "\n",
    "Un pipeline sencillo puede ser el siguiente:\n",
    "\n",
    "```python\n",
    "from sklearn.pipline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), SVC())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.predict(X_test)\n",
    "```\n",
    "En donde, por medio de la utilizacion de una funcion particular \"make_pipeline()\", se le indica con que método debe estandarizar los valores y con que algoritmo se va a trabajar, en este caso un \"SVC()\" (Support Vector Classifier), esto es, una SVM específica para problemas de clasificación.\n",
    "Luego se le indica que entrene con un subconjunto de los datos originales y que prediga los resultados de un subconjunto de datos de prueba.\n",
    "\n",
    "A su vez, hay pasos anteriores a esta utilización, como por ejemplo la división del conjunto de valores que se puede realizar como sigue:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "```\n",
    "En donde la función \"train_test_split()\", recibe como parámetros los datos originales (Tanto los valores originales como las clases a las que estos corresponden), el tamaño en porcentaje del conjunto de prueba resultante y el valor random_state que sirve de semilla para generar un muestreo al azar que le indica al algoritmo como hacer la división"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.3. Clustering\n",
    "\n",
    "#### 1. Describa de los siguientes métodos de clustering cual es la métrica utilizada, sus casos de uso y su escalabilidad en la implementación.\n",
    "#### a) K-Means\n",
    "> **Métrica:** Distancia entre puntos.\n",
    "\n",
    "> **Casos de uso:** Propósito general, donde sean requeridos pocos clusters para resolver problemas. Típicamente encontrándose de a pares y en superficies planas.\n",
    "\n",
    "> **Escalabilidad:** Soporta un alto número de muestras y un nivel medio de clusters (MiniBatchKmeans)\n",
    "\n",
    ">MiniBatchKmeans:  es una variante de KMeans. Ésta utiliza mini-batches para reducir el tiempo de cómputo, mientras intenta optimizar la misma funcion objetivo. \n",
    "\n",
    ">Los mini-batches son subsets de datos de entrada, muestreados al azar en cada iteracion de entrenamiento. Estos mini-batches, reducen la cantidad de computación requerida para converger a una solucion local. \n",
    "\n",
    "> En contraste con otros algoritmos que reducen el tiempo de convergencia de k-means, éste produce resultados que en lo general son ligeramente peores que el algoritmo estándar.\n",
    "\n",
    "#### b) Mean-Shift\n",
    "> **Métrica:** Distancia entre puntos.\n",
    "\n",
    "> **Casos de uso:** Se utilizan cuando es necesaria una gran cantidad de clusters para resolver los problemas (cantidades impares y geometrías no planas).\n",
    "\n",
    "> **Escalabilidad:** Se ve limitado en cuanto a la cantidad de muestras.\n",
    "\n",
    "#### c) Gaussian mixtures\n",
    "> **Métrica:** Distancia de Mahalanobis, Distancia entre centros.\n",
    "\n",
    "> **Casos de uso:** Geometrías planas, bueno para estimaciones de densidad.\n",
    "\n",
    "> **Escalabilidad:** No escala al tener en cuenta el numero de muestras y el numero de clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Llevar una imagen cualquiera (recibida por linea de comando) a 80 colores con el método de clustering K-Means e imprima los porcentajes de estos colores en la imagen resultante. Qué aplicación le daría a este algoritmo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cluster import KMeans\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = plt.imread(\"data/img.jpg\")\n",
    "original_colors = np.unique(img)\n",
    "#plt.imshow(img)\n",
    "n_colors = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = img.astype(dtype=np.float64)/255\n",
    "print img #Se ven los valores RGB llevados a la escala 0..1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w, h, d = original_shape = tuple(img.shape)\n",
    "assert d == 3\n",
    "image_array = np.reshape(img, (w * h, d))\n",
    "print image_array.shape\n",
    "print image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_array_sample = shuffle(image_array, random_state=0)[:1000]\n",
    "kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\n",
    "labels = kmeans.predict(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recreate_image(codebook, labels, w, h):\n",
    "    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
    "    d = codebook.shape[1]\n",
    "    image = np.zeros((w, h, d))\n",
    "    label_idx = 0\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            image[i][j] = codebook[labels[label_idx]]\n",
    "            label_idx += 1\n",
    "    return image\n",
    "\n",
    "def print_img(image, title, figure):\n",
    "    plt.figure(figure)\n",
    "    plt.clf()\n",
    "    ax = plt.axes([0, 0, 1, 1])\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "title1 = 'Original image ({} colors)'.format(original_colors.shape[0])\n",
    "title2 = 'Quantized image ({} colors, K-Means)'.format(n_colors)\n",
    "\n",
    "print_img(img, title1, 1)\n",
    "print_img(recreate_image(kmeans.cluster_centers_, labels, w, h), title2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totales = []\n",
    "for l in np.unique(labels):\n",
    "    totales.append(np.extract(labels==l, labels).shape[0])\n",
    "\n",
    "porcentajes = [((t * 100.00) / len(labels)) for t in totales]\n",
    "\n",
    "pt = PrettyTable()\n",
    "valores = [\"%.3f %%\" % (val) for val in porcentajes]\n",
    "\n",
    "pt.add_column('Cluster', range(len(valores)))\n",
    "pt.add_column('Porcentajes', valores)\n",
    "\n",
    "print pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Aplicación\n",
    "\n",
    "La aplicación que se le puede dar a los algoritmos de clustering, se encuentra detallada en el inciso **1.2** de este documento, pero en síntesis, se utilizan para clasificar o agrupar datos cuando se desconocen de antemano las categorías o grupos a las que estos pertenecen. Debido a esta característica, se dice que son no supervisados.\n",
    "\n",
    "Intentan responder como es que ciertos Objetos (casos) pertenecen o “caen” naturalmente en cierto número de clases o grupos, de tal manera que estos objetos comparten ciertas características.\n",
    "Pero esta definición asume que los objetos pueden dividirse, razonablemente, en grupos que contienen objetos similares. Si tal división existe, ésta puede estar oculta y debe ser descubierta. \n",
    "Éste es el objetivo principal de las técnicas de clustering. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. Aplicar el método de Mean-Shift sobre el dataset IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "#cargamos los datos de iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.3, n_samples=50)\n",
    "ms = MeanShift(bandwidth)\n",
    "ms.fit(X)\n",
    "centers = ms.cluster_centers_ #luego de entrenar, estima los centros\n",
    "labels = ms.labels_ # luego los labels para cada punto\n",
    "labels_unique = np.unique(labels) #nos dice cuantos labels hay\n",
    "n_clusters_ = len(labels_unique) #en base a ello, estima la cantidad de clusters que fueron necesarios\n",
    "print n_clusters_\n",
    "# mostramos los puntos y sus clusters (centros incluidos)\n",
    "colors = ['b','g','c','m','y','k']*3\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    my_members = (labels == k)\n",
    "    cluster_center = centers[k]\n",
    "    plt.plot(X[my_members, 0], X[my_members, 1], col + 'o')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], u'o', markerfacecolor='r',\n",
    "              markersize=10)\n",
    "\n",
    "\n",
    "plt.title('Numero estimado de clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Clasificación\n",
    "\n",
    "**1. Comente dos kernels posibles para una SVM, cuales son sus diferencias, y que tan importante es la elección de estos a la hora de resolver un problema. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** 2. Implemente sobre un mismo dataset a elección una SVM con dos kernels diferentes y grafíquelos. Qué diferencias encontró? Para que tipo de distribución es útil un kernel lineal?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Clasifique por calidad a los vinos encontrados en el archivo winequality red.csv, cuales son los valores promedio para obtener un vino de calidad 6?. Grafique la función de decisión obtenida. Qué clasificador utilizó? Realice el mismo análisis con otro método y compare. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Regresión\n",
    "\n",
    "** 1. Liberty Mutual Group es una aseguradora mundial que desea predecir el score Hazard para saber si es rentable asegurar a una propiedad o no, este puntaje es un valor continuo que se saca de ciertas características dadas de la propiedad (ver archivo csv). ** \n",
    "\n",
    "**Realice una red neuronal para predecir automáticamente la puntuación de riesgo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Implemente una solución con SVR (Support Vector Regression) y compare los resultados.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "from sklearn import svm, preprocessing\n",
    "\n",
    "#Obtenemos los datos de entrenamiento\n",
    "train = read_csv('data/1.5/train.csv')\n",
    "data = train.ix[:, train.columns != 'Hazard'][:1000] #Quitamos la columna Hazard\n",
    "X_train = data.ix[:, data.columns != 'Id'][:1000] #Quitamos la columna Id\n",
    "y_train = train['Hazard'][:1000]\n",
    "\n",
    "#Creamos algunas funciones de utilidad\n",
    "def train_encoder(encoder):\n",
    "    \"\"\"\n",
    "        Crea una lista con las clases presentes en el archivo de entrenamiento:\n",
    "        Letras: A..S,W,Y\n",
    "        Luego entrena el encoder con dichas clases y lo retorna\n",
    "    \"\"\"\n",
    "    classes_ = [s for s in string.uppercase[:19]] #Representa todas las letras presentes en el archivo A-S + W + Y\n",
    "    classes_.append('W')\n",
    "    classes_.append('Y')\n",
    "    return encoder.fit(classes_)\n",
    "\n",
    "\n",
    "def get_alpha(features, dataset, encoder):\n",
    "    \"\"\"\n",
    "        Identificados los features (nombres de las columnas) cuyo valor no es numerico,\n",
    "        esta funcion aplica una codificacion con el encoder pasado como parametro\n",
    "        y devuelve una lista con los datos transformados, en sus respectivas columnas.\n",
    "    \"\"\"\n",
    "    alpha_cols = []\n",
    "    for f in features:\n",
    "        col = dataset.ix[:, dataset.columns == f]\n",
    "        alpha_cols.append(np.asarray(col))\n",
    "    encoded_values = [] \n",
    "    for col in alpha_cols:\n",
    "        result = encoder.transform(col.ravel())\n",
    "        encoded_values.append(result)\n",
    "    return encoded_values\n",
    "\n",
    "def get_non_alpha(features, dataset):\n",
    "    \"\"\"\n",
    "        Identificados los features (nombres de las columnas) cuyo valor es numerico,\n",
    "        esta funcion separa dichas columnas del dataset original y las devuelve.\n",
    "    \"\"\"\n",
    "    non_alpha_cols = []\n",
    "    for f in features:\n",
    "        col = np.asarray(dataset.ix[:, dataset.columns == f])\n",
    "        non_alpha_cols.append(col.ravel())\n",
    "    return non_alpha_cols\n",
    "\n",
    "def get_features_names(values, columns):\n",
    "    \"\"\"\n",
    "        Dados los valores, retorna dos listas:\n",
    "        * alpha_features: que contendra los titulos de las features con letras\n",
    "        * non_alpha_features: que contendra los titulos de las features con numeros\n",
    "    \"\"\"\n",
    "    alpha_features = [] \n",
    "    non_alpha_features = []\n",
    "    for i, v in enumerate(values):\n",
    "        if not str(v).isdigit():\n",
    "            alpha_features.append(columns[i])\n",
    "        else:\n",
    "            non_alpha_features.append(columns[i])\n",
    "    return alpha_features, non_alpha_features\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Recibe el dataset original y crea uno nuevo habiendo codificado lo que corresponde,\n",
    "        es decir, los datos alfanumericos.\n",
    "    \"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le = train_encoder(le)\n",
    "    \n",
    "    a_features, na_features = get_features_names(dataset.values[0], dataset.columns) \n",
    "\n",
    "    na_dataset = get_non_alpha(na_features, dataset)\n",
    "    a_dataset = get_alpha(a_features, dataset, le)\n",
    "    \n",
    "    df = {}\n",
    "    for i, f in enumerate(a_features):\n",
    "        df.update({f: a_dataset[i]})\n",
    "\n",
    "    for i, f in enumerate(na_features):\n",
    "        df.update({f: na_dataset[i]})\n",
    "\n",
    "    new_dataset = DataFrame(df)\n",
    "    return new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocesamos los datos y se los pasamos a un SVR para que entrene\n",
    "df_train = preprocess_data(X_train)\n",
    "svr = svm.SVR(kernel='linear')\n",
    "svr.fit(df_train.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|  Id  | Predicted hazard |\n",
      "+------+------------------+\n",
      "|  6   |  1.81091931514   |\n",
      "|  7   |  4.03662808268   |\n",
      "|  8   |  3.50590198338   |\n",
      "|  9   |  2.54277646945   |\n",
      "|  10  |  2.28883246764   |\n",
      "|  11  |  2.58016709243   |\n",
      "|  13  |  2.77876985181   |\n",
      "|  14  |  4.82784705535   |\n",
      "|  16  |  2.41216969434   |\n",
      "|  17  |  5.49260462874   |\n",
      "|  18  |   2.4001266488   |\n",
      "|  20  |  3.02160552677   |\n",
      "|  27  |  2.61896054362   |\n",
      "|  28  |  4.61848101157   |\n",
      "|  29  |  2.18004211329   |\n",
      "|  30  |  3.28313303644   |\n",
      "|  34  |  3.97188050826   |\n",
      "|  35  |  3.21820989583   |\n",
      "|  36  |  2.89198945413   |\n",
      "|  37  |  5.35567590401   |\n",
      "|  38  |  1.25031383877   |\n",
      "|  40  |  4.22593615486   |\n",
      "|  42  |  3.05195148539   |\n",
      "|  46  |  2.73979851557   |\n",
      "|  47  |  2.49953381857   |\n",
      "|  48  |  1.90546247254   |\n",
      "|  49  |  4.65417528724   |\n",
      "|  51  |   2.4730813754   |\n",
      "|  52  |  3.18367617445   |\n",
      "|  53  |  1.69601667773   |\n",
      "|  54  |  1.86720426878   |\n",
      "|  55  |   3.0806890743   |\n",
      "|  56  |  3.36284499575   |\n",
      "|  57  |  2.51599006131   |\n",
      "|  60  |  2.15730812392   |\n",
      "|  61  |   2.6098486642   |\n",
      "|  64  |  3.94710286902   |\n",
      "|  65  |  3.28399391015   |\n",
      "|  66  |  2.47366496778   |\n",
      "|  68  |  1.82780648702   |\n",
      "|  70  |   1.8493953682   |\n",
      "|  71  |  1.94533425025   |\n",
      "|  75  |   3.7288539564   |\n",
      "|  76  |  2.22757486463   |\n",
      "|  78  |  2.34990747009   |\n",
      "|  80  |  3.84050686739   |\n",
      "|  81  |  2.89504060954   |\n",
      "|  82  |  1.50358910602   |\n",
      "|  83  |   3.0347281019   |\n",
      "|  85  |  1.20003283796   |\n",
      "|  86  |  1.45290982527   |\n",
      "|  87  |  2.55156499292   |\n",
      "|  89  |  1.86804138394   |\n",
      "|  90  |  3.30366525293   |\n",
      "|  91  |  2.91386408112   |\n",
      "|  92  |   2.6099485186   |\n",
      "|  95  |  1.89706840363   |\n",
      "|  97  |  3.31444501229   |\n",
      "|  99  |  1.79945451103   |\n",
      "| 101  |  1.70838507326   |\n",
      "| 102  |  3.83738010077   |\n",
      "| 103  |  2.32515840529   |\n",
      "| 104  |  2.85763017612   |\n",
      "| 107  |   4.0385609643   |\n",
      "| 108  |  3.79117998251   |\n",
      "| 111  |  2.20052918864   |\n",
      "| 112  |  3.89117459749   |\n",
      "| 115  |  3.24111855312   |\n",
      "| 119  |  1.87047429933   |\n",
      "| 121  |  2.01151185285   |\n",
      "| 124  |  2.32881515319   |\n",
      "| 128  |  3.30966508353   |\n",
      "| 130  |  3.83373916114   |\n",
      "| 131  |  3.90816194645   |\n",
      "| 133  |  4.91368060512   |\n",
      "| 134  |  3.61672911724   |\n",
      "| 137  |  3.44237973264   |\n",
      "| 138  |  2.69057370658   |\n",
      "| 142  |  4.42599595932   |\n",
      "| 143  |  3.51106957954   |\n",
      "| 144  |  2.96336930229   |\n",
      "| 145  |  3.55911786586   |\n",
      "| 147  |   3.8723514311   |\n",
      "| 148  |  4.55800662185   |\n",
      "| 152  |   1.2858541208   |\n",
      "| 154  |  3.56393322747   |\n",
      "| 158  |  1.56967527876   |\n",
      "| 159  |  2.86251166548   |\n",
      "| 160  |   2.7281507642   |\n",
      "| 161  |  3.25637650957   |\n",
      "| 164  |  2.92315327548   |\n",
      "| 165  |  3.12691634915   |\n",
      "| 166  |  1.72723423529   |\n",
      "| 167  |  1.60581569714   |\n",
      "| 169  |  3.69046068324   |\n",
      "| 170  |  2.67620013557   |\n",
      "| 171  |  4.44368701897   |\n",
      "| 175  |  1.52124691839   |\n",
      "| 177  |  3.83822294889   |\n",
      "| 178  |   4.8614142041   |\n",
      "| 180  |  2.86170317812   |\n",
      "| 182  |  2.81736995451   |\n",
      "| 184  |  3.42977893072   |\n",
      "| 185  |   4.1010657882   |\n",
      "| 186  |  2.57570453294   |\n",
      "| 190  |   2.7792820828   |\n",
      "| 191  |  3.63709419957   |\n",
      "| 192  |  2.46608325789   |\n",
      "| 195  |  5.64797567478   |\n",
      "| 196  |  4.05018364687   |\n",
      "| 197  |  4.14884157728   |\n",
      "| 198  |  2.41443792701   |\n",
      "| 199  |  1.64567154596   |\n",
      "| 200  |  1.48781999464   |\n",
      "| 204  |  2.08367794833   |\n",
      "| 205  |  3.74244240404   |\n",
      "| 207  |  2.08250805252   |\n",
      "| 208  |  2.48008293801   |\n",
      "| 209  |  2.85277244075   |\n",
      "| 211  |  3.21508004214   |\n",
      "| 216  |  5.01625843255   |\n",
      "| 217  |  1.96065158281   |\n",
      "| 220  |  4.37932747866   |\n",
      "| 223  |  4.14738196105   |\n",
      "| 224  |  2.66393356622   |\n",
      "| 230  |  3.47472976827   |\n",
      "| 232  |   3.0533793871   |\n",
      "| 235  |  2.52482940168   |\n",
      "| 236  |  3.96203341934   |\n",
      "| 238  |   1.4378437048   |\n",
      "| 240  |  1.36296195118   |\n",
      "| 242  |  3.10368137604   |\n",
      "| 244  |  6.62031297404   |\n",
      "| 245  |  3.18505501047   |\n",
      "| 248  |  3.40556080075   |\n",
      "| 251  |   1.4473806685   |\n",
      "| 256  |  2.62527189134   |\n",
      "| 258  |  3.57071367386   |\n",
      "| 259  |  2.08238326171   |\n",
      "| 261  |   3.9385869057   |\n",
      "| 267  |   2.9448305955   |\n",
      "| 268  |  3.42003228621   |\n",
      "| 270  |  2.76400617301   |\n",
      "| 272  |  4.13466124973   |\n",
      "| 273  |  2.64636937459   |\n",
      "| 275  |  2.67637354461   |\n",
      "| 276  |  2.29690334202   |\n",
      "| 277  |  4.26801233815   |\n",
      "| 279  |  1.53974883036   |\n",
      "| 283  |  3.66290708005   |\n",
      "| 284  |  2.58685694402   |\n",
      "| 286  |  2.55660618399   |\n",
      "| 287  |   2.1576199095   |\n",
      "| 288  |  2.90284165184   |\n",
      "| 289  |  3.91508498718   |\n",
      "| 292  |  2.49283339245   |\n",
      "| 293  |  2.80140689874   |\n",
      "| 295  |  3.43476916872   |\n",
      "| 297  |   3.5420783994   |\n",
      "| 301  |  3.04146997773   |\n",
      "| 303  |  2.72100876189   |\n",
      "| 304  |  2.22110024641   |\n",
      "| 305  |  2.31102354037   |\n",
      "| 308  |  1.94457402428   |\n",
      "| 309  |  3.59250556529   |\n",
      "| 313  |  3.72196571939   |\n",
      "| 314  |  3.99043096212   |\n",
      "| 315  |  3.77389743487   |\n",
      "| 316  |  3.91388253294   |\n",
      "| 318  |  4.39315818249   |\n",
      "| 320  |  4.08395681994   |\n",
      "| 324  |  2.38983773389   |\n",
      "| 325  |  3.30861683343   |\n",
      "| 326  |  2.98837049741   |\n",
      "| 327  |  3.76670123127   |\n",
      "| 328  |  5.11805401739   |\n",
      "| 332  |  2.07517482753   |\n",
      "| 333  |  2.47431797435   |\n",
      "| 336  |  2.64218442199   |\n",
      "| 339  |  3.15764532183   |\n",
      "| 340  |  2.46516307664   |\n",
      "| 342  |  2.30766071763   |\n",
      "| 344  |  4.10818957189   |\n",
      "| 345  |   2.7488796571   |\n",
      "| 346  |  1.57997163597   |\n",
      "| 349  |  2.86596259067   |\n",
      "| 353  |  2.97904186891   |\n",
      "| 355  |  3.43632454307   |\n",
      "| 360  |   2.0894017356   |\n",
      "| 363  |  2.69650866571   |\n",
      "| 366  |   2.0939689183   |\n",
      "| 367  |  1.74327251018   |\n",
      "| 369  |  1.61402934976   |\n",
      "| 371  |  3.69676450192   |\n",
      "| 372  |  0.756242735252  |\n",
      "| 374  |  3.52313662937   |\n",
      "| 375  |  2.26183785721   |\n",
      "| 378  |  2.09570335783   |\n",
      "| 379  |  1.19508267228   |\n",
      "| 381  |  2.18973705682   |\n",
      "| 382  |  3.10969753019   |\n",
      "| 384  |  1.37162566573   |\n",
      "| 385  |  2.59216663486   |\n",
      "| 387  |  5.00948860821   |\n",
      "| 388  |  2.11256616987   |\n",
      "| 389  |  3.75275784565   |\n",
      "| 390  |  5.01443396882   |\n",
      "| 393  |  3.03728765037   |\n",
      "| 394  |  3.68896862777   |\n",
      "| 395  |  2.14212170089   |\n",
      "| 397  |  1.81195120708   |\n",
      "| 398  |  3.30796675348   |\n",
      "| 399  |  3.59840680494   |\n",
      "| 403  |  3.02383381317   |\n",
      "| 404  |  2.06157300618   |\n",
      "| 406  |   2.8141823318   |\n",
      "| 407  |  2.35484584763   |\n",
      "| 410  |  1.18900379585   |\n",
      "| 411  |  3.92212611936   |\n",
      "| 412  |  1.86802188463   |\n",
      "| 413  |  3.21056818199   |\n",
      "| 417  |  1.99162942459   |\n",
      "| 419  |  3.52474272509   |\n",
      "| 422  |  4.35182620547   |\n",
      "| 425  |   3.7496901016   |\n",
      "| 426  |  1.90677898133   |\n",
      "| 427  |  3.26620610941   |\n",
      "| 432  |  1.52572709553   |\n",
      "| 434  |  2.15165677148   |\n",
      "| 437  |  2.43968887039   |\n",
      "| 438  |  2.57253250814   |\n",
      "| 440  |  2.04339487417   |\n",
      "| 441  |  3.89083308194   |\n",
      "| 443  |  1.02321341247   |\n",
      "| 445  |  2.76376554985   |\n",
      "| 447  |  3.27688940432   |\n",
      "| 449  |  0.575041032759  |\n",
      "| 450  |  3.23221042732   |\n",
      "| 451  |  2.92958216573   |\n",
      "| 453  |  3.67469535461   |\n",
      "| 455  |  2.63407294714   |\n",
      "| 456  |  2.70973238138   |\n",
      "| 459  |  3.41119593268   |\n",
      "| 461  |  1.98151564305   |\n",
      "| 463  |  2.04499386117   |\n",
      "| 466  |  3.99616986808   |\n",
      "| 467  |  1.38252203211   |\n",
      "| 469  |  1.48953673696   |\n",
      "| 470  |  5.19078201288   |\n",
      "| 472  |  2.52320276186   |\n",
      "| 477  |   2.1797327832   |\n",
      "| 481  |  2.03753624057   |\n",
      "| 485  |  1.77041572719   |\n",
      "| 492  |  2.08917070602   |\n",
      "| 494  |  3.22638363254   |\n",
      "| 497  |  2.07330618476   |\n",
      "| 503  |  2.57278410879   |\n",
      "| 504  |  3.02299193325   |\n",
      "| 507  |  1.64761928377   |\n",
      "| 509  |  1.98576090719   |\n",
      "| 511  |  1.68013326329   |\n",
      "| 512  |  2.60360242421   |\n",
      "| 513  |  0.932677717151  |\n",
      "| 516  |  3.82688702048   |\n",
      "| 517  |  1.23584086675   |\n",
      "| 518  |  2.59343612786   |\n",
      "| 522  |  3.19222978294   |\n",
      "| 525  |  2.12517634635   |\n",
      "| 526  |  2.08543131481   |\n",
      "| 527  |  3.30671204813   |\n",
      "| 529  |  2.03898690744   |\n",
      "| 534  |  3.36868318138   |\n",
      "| 538  |  -0.11882263476  |\n",
      "| 540  |  3.45175176627   |\n",
      "| 541  |  1.54932612026   |\n",
      "| 542  |  3.66336626807   |\n",
      "| 543  |  3.77185771936   |\n",
      "| 544  |  1.81489947598   |\n",
      "| 546  |  4.14047585736   |\n",
      "| 547  |  0.519248588715  |\n",
      "| 550  |  3.08825354083   |\n",
      "| 554  |  3.70564584721   |\n",
      "| 556  |  2.00773009256   |\n",
      "| 558  |  2.01453701006   |\n",
      "| 559  |  0.790032725955  |\n",
      "| 560  |  3.38633069146   |\n",
      "| 564  |  2.58377526128   |\n",
      "| 565  |  1.14862432887   |\n",
      "| 566  |  3.34575404935   |\n",
      "| 567  |  4.23179217803   |\n",
      "| 568  |  2.99907533123   |\n",
      "| 570  |  2.72495379656   |\n",
      "| 571  |   1.3465586251   |\n",
      "| 575  |  1.07083682231   |\n",
      "| 576  |  4.12532463067   |\n",
      "| 585  |  1.29198282052   |\n",
      "| 586  |  2.60017132102   |\n",
      "| 589  |  2.82242032792   |\n",
      "| 590  |  2.68160163564   |\n",
      "| 591  |  3.47478286817   |\n",
      "| 593  |  1.62250289536   |\n",
      "| 594  |  2.94715954768   |\n",
      "| 595  |  1.55873950058   |\n",
      "| 596  |  2.14399557431   |\n",
      "| 597  |  4.31555706041   |\n",
      "| 598  |  3.33939475487   |\n",
      "| 599  |   2.9407627174   |\n",
      "| 600  |  3.35351744731   |\n",
      "| 601  |  3.07540910253   |\n",
      "| 609  |   2.4441510645   |\n",
      "| 613  |  2.53432203708   |\n",
      "| 619  |  3.21658161968   |\n",
      "| 620  |  2.63750111873   |\n",
      "| 621  |  2.96143816947   |\n",
      "| 622  |  2.32433374632   |\n",
      "| 623  |  3.04630039008   |\n",
      "| 625  |  4.91968288077   |\n",
      "| 626  |  2.00378803124   |\n",
      "| 628  |  1.50014335256   |\n",
      "| 629  |  2.93615391919   |\n",
      "| 633  |  3.55575981228   |\n",
      "| 634  |  2.15301079684   |\n",
      "| 635  |  4.40817981145   |\n",
      "| 636  |  3.18585209825   |\n",
      "| 637  |  4.07893204347   |\n",
      "| 638  |  1.95462001648   |\n",
      "| 639  |  2.05487486759   |\n",
      "| 640  |  3.19805235718   |\n",
      "| 643  |  3.02495457337   |\n",
      "| 645  |  5.59841484432   |\n",
      "| 646  |  2.88755386043   |\n",
      "| 647  |  0.831806410714  |\n",
      "| 649  |  1.69052094074   |\n",
      "| 650  |  0.181174558368  |\n",
      "| 654  |  3.51256770006   |\n",
      "| 656  |  1.44428576328   |\n",
      "| 657  |   2.8624390445   |\n",
      "| 661  |  2.15694578613   |\n",
      "| 662  |  1.53112247911   |\n",
      "| 663  |  3.70155114318   |\n",
      "| 664  |  1.49998180166   |\n",
      "| 667  |  2.84502409788   |\n",
      "| 668  |  2.86704507572   |\n",
      "| 672  |  1.53365054943   |\n",
      "| 673  |  3.27930722162   |\n",
      "| 675  |  3.31667206641   |\n",
      "| 676  |  2.44385235995   |\n",
      "| 677  |  2.74940626023   |\n",
      "| 678  |  2.86794713119   |\n",
      "| 679  |  3.20451678363   |\n",
      "| 682  |  1.33289382608   |\n",
      "| 684  |  3.31673889812   |\n",
      "| 686  |  2.51138813666   |\n",
      "| 687  |  3.29433554157   |\n",
      "| 688  |  2.32679090902   |\n",
      "| 689  |  2.61389173069   |\n",
      "| 690  |  3.91735300659   |\n",
      "| 691  |  2.58053580057   |\n",
      "| 693  |  1.94948392979   |\n",
      "| 694  |  2.13180715751   |\n",
      "| 695  |  2.81512555954   |\n",
      "| 696  |  2.15974674473   |\n",
      "| 699  |  4.67998908182   |\n",
      "| 702  |  3.69727049532   |\n",
      "| 703  |  2.00876257168   |\n",
      "| 704  |  1.67598087445   |\n",
      "| 705  |  1.78848430693   |\n",
      "| 708  |  2.83768449315   |\n",
      "| 709  |  1.24356543789   |\n",
      "| 710  |  2.79800898116   |\n",
      "| 711  |  2.45334819519   |\n",
      "| 712  |  3.14431094611   |\n",
      "| 713  |  2.96704782368   |\n",
      "| 714  |  2.96236649019   |\n",
      "| 724  |  3.87718990655   |\n",
      "| 725  |  2.62483324943   |\n",
      "| 727  |  1.09932207962   |\n",
      "| 729  |  3.10283025619   |\n",
      "| 731  |  3.27404452528   |\n",
      "| 732  |   3.5215184392   |\n",
      "| 735  |  1.50170005388   |\n",
      "| 736  |  3.38122118709   |\n",
      "| 738  |   3.2341038847   |\n",
      "| 739  |  3.63727647069   |\n",
      "| 741  |  4.31646513495   |\n",
      "| 744  |  1.38152262955   |\n",
      "| 745  |  2.52648492503   |\n",
      "| 749  |  3.59795161924   |\n",
      "| 750  |  3.03397445573   |\n",
      "| 753  |  2.02752402352   |\n",
      "| 754  |  3.97327094833   |\n",
      "| 756  |  4.03092365121   |\n",
      "| 759  |  2.23192832134   |\n",
      "| 763  |  2.91813297768   |\n",
      "| 764  |  2.64574695313   |\n",
      "| 765  |  3.47783639714   |\n",
      "| 770  |  2.56381125699   |\n",
      "| 773  |   2.8467772112   |\n",
      "| 774  |  0.427402744432  |\n",
      "| 778  |  2.02165625941   |\n",
      "| 780  |  2.84084733068   |\n",
      "| 781  |  2.39690337251   |\n",
      "| 783  |  4.36169245039   |\n",
      "| 784  |  3.43482023763   |\n",
      "| 790  |  3.39159124662   |\n",
      "| 791  |  2.00304093514   |\n",
      "| 792  |  1.43455269348   |\n",
      "| 793  |  1.53993107176   |\n",
      "| 794  |  3.34363561768   |\n",
      "| 795  |   1.6633497455   |\n",
      "| 796  |  3.46963469716   |\n",
      "| 801  |  3.79512392598   |\n",
      "| 802  |  3.38491331624   |\n",
      "| 805  |  1.92042446333   |\n",
      "| 808  |  2.05671670863   |\n",
      "| 809  |  4.38161951836   |\n",
      "| 813  |  2.89855713979   |\n",
      "| 814  |  5.08956693072   |\n",
      "| 815  |  1.59846883546   |\n",
      "| 816  |  3.72973589004   |\n",
      "| 817  |  1.93775257035   |\n",
      "| 821  |  3.04855131644   |\n",
      "| 822  |  1.77475538548   |\n",
      "| 823  |   1.8070168524   |\n",
      "| 825  |  2.61286162343   |\n",
      "| 826  |  2.60275188181   |\n",
      "| 830  |  1.79185270512   |\n",
      "| 832  |  3.91486954047   |\n",
      "| 833  |  3.31813398047   |\n",
      "| 835  |  3.61191107026   |\n",
      "| 837  |  2.79816930539   |\n",
      "| 838  |  1.73155100321   |\n",
      "| 839  |  3.30252014318   |\n",
      "| 841  |  4.01285081162   |\n",
      "| 842  |  2.77159744775   |\n",
      "| 844  |  1.89037983089   |\n",
      "| 853  |  1.70591745569   |\n",
      "| 855  |  1.97495103691   |\n",
      "| 858  |  2.14959147369   |\n",
      "| 859  |  2.70786606005   |\n",
      "| 860  |  1.08948999243   |\n",
      "| 861  |  4.34757947025   |\n",
      "| 862  |   2.2583200106   |\n",
      "| 868  |  3.45713428663   |\n",
      "| 869  |   3.3805604947   |\n",
      "| 874  |  4.33554125175   |\n",
      "| 875  |  1.97014732979   |\n",
      "| 877  |  2.37363797722   |\n",
      "| 879  |  3.26289952248   |\n",
      "| 885  |  3.45881517501   |\n",
      "| 886  |  3.07656225514   |\n",
      "| 887  |  2.71197926431   |\n",
      "| 892  |  2.97244776673   |\n",
      "| 894  |  2.92574278568   |\n",
      "| 896  |  3.48820976118   |\n",
      "| 898  |  2.75546774728   |\n",
      "| 899  |  1.59537595899   |\n",
      "| 900  |  2.63326366519   |\n",
      "| 901  |   1.6609030956   |\n",
      "| 903  |  3.22093772415   |\n",
      "| 904  |  3.58949836624   |\n",
      "| 906  |  1.57207212868   |\n",
      "| 907  |  3.57773045423   |\n",
      "| 908  |    3.53512092    |\n",
      "| 909  |  3.53774742497   |\n",
      "| 915  |  2.91716678615   |\n",
      "| 916  |  3.11791558679   |\n",
      "| 917  |  3.30058470684   |\n",
      "| 921  |  1.04283944634   |\n",
      "| 922  |  3.84156192787   |\n",
      "| 923  |  3.44590878588   |\n",
      "| 924  |  2.07905937307   |\n",
      "| 927  |  3.22363592869   |\n",
      "| 929  |  2.19658590879   |\n",
      "| 932  |  3.51552606479   |\n",
      "| 934  |  1.20215631474   |\n",
      "| 935  |  3.91956741873   |\n",
      "| 937  |  3.10730556176   |\n",
      "| 940  |  3.47718268365   |\n",
      "| 944  |  4.08884328926   |\n",
      "| 946  |   4.9293192699   |\n",
      "| 948  |  4.53769172013   |\n",
      "| 950  |  4.36186331405   |\n",
      "| 952  |  2.55761634337   |\n",
      "| 953  |  3.68506760495   |\n",
      "| 956  |  1.85686795185   |\n",
      "| 959  |  2.78924424897   |\n",
      "| 960  |  3.03924954404   |\n",
      "| 961  |  2.66389541391   |\n",
      "| 962  |  3.46582668715   |\n",
      "| 966  |  2.87941914719   |\n",
      "| 968  |  3.53878161203   |\n",
      "| 969  |  3.51159315668   |\n",
      "| 971  |  2.75851377314   |\n",
      "| 972  |  1.75809945364   |\n",
      "| 978  |   2.4273035736   |\n",
      "| 983  |  4.21842240472   |\n",
      "| 984  |  3.36895069334   |\n",
      "| 985  |  2.83995213013   |\n",
      "| 987  |  1.92663824052   |\n",
      "| 988  |  2.83792182938   |\n",
      "| 989  |  4.87203068871   |\n",
      "| 991  |  2.54254936764   |\n",
      "| 992  |  3.53337445385   |\n",
      "| 993  |   1.5032543586   |\n",
      "| 997  |  3.11471302317   |\n",
      "| 999  |  1.62789516984   |\n",
      "| 1003 |   1.286573082    |\n",
      "| 1006 |  2.80294830722   |\n",
      "| 1007 |  3.93274333385   |\n",
      "| 1013 |  3.11200177998   |\n",
      "| 1015 |  1.52280337058   |\n",
      "| 1016 |  1.69681887373   |\n",
      "| 1017 |  2.02517576163   |\n",
      "| 1018 |  3.17338969937   |\n",
      "| 1019 |  3.12477544306   |\n",
      "| 1020 |  2.16921434148   |\n",
      "| 1021 |  3.26095651233   |\n",
      "| 1023 |  1.84009022699   |\n",
      "| 1025 |  1.73365165433   |\n",
      "| 1027 |  2.79894038688   |\n",
      "| 1028 |  3.41381232221   |\n",
      "| 1029 |  2.53608971518   |\n",
      "| 1032 |  4.03103724695   |\n",
      "| 1035 |  2.64186539008   |\n",
      "| 1039 |   3.7875726847   |\n",
      "| 1040 |  3.98549071746   |\n",
      "| 1041 |  1.56467225884   |\n",
      "| 1043 |  2.92253541493   |\n",
      "| 1045 |  3.37543903483   |\n",
      "| 1046 |  5.55133658365   |\n",
      "| 1049 |  3.00617759264   |\n",
      "| 1054 |  5.47227634747   |\n",
      "| 1056 |  2.89460598929   |\n",
      "| 1057 |  0.628442683183  |\n",
      "| 1058 |  3.08143366266   |\n",
      "| 1060 |  2.88805956441   |\n",
      "| 1061 |  3.48530174724   |\n",
      "| 1062 |  3.65147489204   |\n",
      "| 1063 |    2.96401199    |\n",
      "| 1066 |  3.11384884288   |\n",
      "| 1071 |  2.57362198211   |\n",
      "| 1072 |  2.96989508103   |\n",
      "| 1076 |  4.37897857159   |\n",
      "| 1077 |  4.26661538618   |\n",
      "| 1078 |  2.72156959809   |\n",
      "| 1082 |   3.1832951262   |\n",
      "| 1083 |   2.6161876613   |\n",
      "| 1085 |  2.95128391796   |\n",
      "| 1086 |  4.57150539843   |\n",
      "| 1088 |  2.76614356368   |\n",
      "| 1089 |  5.26725225245   |\n",
      "| 1090 |   2.3653722482   |\n",
      "| 1093 |  4.13942784136   |\n",
      "| 1094 |  3.66405867239   |\n",
      "| 1095 |  2.86229388325   |\n",
      "| 1096 |  5.17357805147   |\n",
      "| 1097 |  3.61736807771   |\n",
      "| 1098 |  3.43591733827   |\n",
      "| 1103 |  2.37199790182   |\n",
      "| 1104 |  3.32091516528   |\n",
      "| 1109 |  2.54042856787   |\n",
      "| 1110 |  3.16336496772   |\n",
      "| 1111 |  3.19779968561   |\n",
      "| 1113 |   1.6050722668   |\n",
      "| 1114 |  1.92837044483   |\n",
      "| 1115 |  2.37230177473   |\n",
      "| 1117 |  2.11751835568   |\n",
      "| 1118 |  1.54030504677   |\n",
      "| 1121 |  2.94768974659   |\n",
      "| 1122 |  1.30619202816   |\n",
      "| 1124 |  3.58745365578   |\n",
      "| 1125 |  2.64104681638   |\n",
      "| 1126 |  3.49786193602   |\n",
      "| 1127 |  3.01558866442   |\n",
      "| 1128 |  2.72960312711   |\n",
      "| 1130 |  1.94709746662   |\n",
      "| 1132 |  3.66834617594   |\n",
      "| 1135 |  3.41827082047   |\n",
      "| 1136 |  3.96308912516   |\n",
      "| 1139 |  5.09944056222   |\n",
      "| 1140 |  2.98354418621   |\n",
      "| 1141 |  2.92045113817   |\n",
      "| 1147 |  2.76107221866   |\n",
      "| 1150 |  4.08199479577   |\n",
      "| 1152 |  1.66513122109   |\n",
      "| 1154 |  3.72756203389   |\n",
      "| 1157 |  1.82556448845   |\n",
      "| 1158 |  1.83931489223   |\n",
      "| 1160 |  4.36119425113   |\n",
      "| 1162 |  2.19012568703   |\n",
      "| 1163 |   1.9466167199   |\n",
      "| 1164 |  2.98008012361   |\n",
      "| 1171 |  2.13941896168   |\n",
      "| 1173 |  1.92148432055   |\n",
      "| 1174 |  3.09553274562   |\n",
      "| 1176 |  1.97586431194   |\n",
      "| 1178 |  3.86382266218   |\n",
      "| 1182 |  3.51445481471   |\n",
      "| 1185 |  2.80401404163   |\n",
      "| 1188 |  1.86367897346   |\n",
      "| 1189 |   1.1883848946   |\n",
      "| 1192 |  3.03177505638   |\n",
      "| 1194 |  2.23557023839   |\n",
      "| 1195 |  2.50984494829   |\n",
      "| 1199 |  2.08595077402   |\n",
      "| 1200 |  1.97369031198   |\n",
      "| 1203 |  4.34583071868   |\n",
      "| 1204 |  4.47173517846   |\n",
      "| 1206 |  4.45652924144   |\n",
      "| 1209 |  3.21628855642   |\n",
      "| 1214 |   4.3873480151   |\n",
      "| 1215 |  3.01028662568   |\n",
      "| 1217 |  2.06543513027   |\n",
      "| 1219 |  4.16974629999   |\n",
      "| 1220 |  2.99001278893   |\n",
      "| 1221 |  1.58327170836   |\n",
      "| 1222 |  4.71551469953   |\n",
      "| 1223 |  2.72794531442   |\n",
      "| 1225 |  5.02217064371   |\n",
      "| 1226 |  2.05591460797   |\n",
      "| 1228 |   3.5188349676   |\n",
      "| 1229 |  1.24122627408   |\n",
      "| 1231 |  1.78904444253   |\n",
      "| 1233 |  1.40898086793   |\n",
      "| 1234 |  3.14663038654   |\n",
      "| 1235 |  1.42588779556   |\n",
      "| 1236 |  3.41494614234   |\n",
      "| 1238 |  4.93893316662   |\n",
      "| 1240 |   3.7495531728   |\n",
      "| 1241 |  3.44902744703   |\n",
      "| 1242 |  3.02958794658   |\n",
      "| 1244 |  1.90253587275   |\n",
      "| 1245 |  1.05516565539   |\n",
      "| 1248 |  1.50779653674   |\n",
      "| 1250 |  2.91936511992   |\n",
      "| 1251 |  1.60745149953   |\n",
      "| 1252 |  4.24674958576   |\n",
      "| 1253 |  4.16738820696   |\n",
      "| 1255 |  2.16921189077   |\n",
      "| 1258 |  3.03364990196   |\n",
      "| 1261 |  4.22434131833   |\n",
      "| 1262 |  3.23578269775   |\n",
      "| 1267 |  3.76015346046   |\n",
      "| 1268 |  2.20090268911   |\n",
      "| 1270 |   1.7480682302   |\n",
      "| 1271 |  3.40394077818   |\n",
      "| 1274 |  3.52531675146   |\n",
      "| 1275 |  2.15489163886   |\n",
      "| 1276 |   3.2689949917   |\n",
      "| 1277 |   2.1201179407   |\n",
      "| 1279 |  2.14166068844   |\n",
      "| 1280 |  2.30678258097   |\n",
      "| 1281 |  3.29111312069   |\n",
      "| 1282 |  3.10376317889   |\n",
      "| 1285 |  2.25054988057   |\n",
      "| 1289 |  2.49017926097   |\n",
      "| 1293 |  0.154243009833  |\n",
      "| 1294 |  4.65155209474   |\n",
      "| 1295 |  3.80092950373   |\n",
      "| 1296 |  2.48567904545   |\n",
      "| 1298 |  3.19598004506   |\n",
      "| 1300 |  2.72051444508   |\n",
      "| 1301 |   3.1298613543   |\n",
      "| 1302 |  1.87036146204   |\n",
      "| 1303 |  1.88308606885   |\n",
      "| 1304 |  4.51452374698   |\n",
      "| 1306 |  4.95717062598   |\n",
      "| 1308 |  3.81263386499   |\n",
      "| 1311 |  4.09355457895   |\n",
      "| 1312 |  0.917778321526  |\n",
      "| 1313 |  4.04292372335   |\n",
      "| 1314 |  3.13893149351   |\n",
      "| 1315 |  2.27540744767   |\n",
      "| 1316 |  2.58419408549   |\n",
      "| 1317 |  4.17926748193   |\n",
      "| 1319 |  3.09415033874   |\n",
      "| 1320 |  1.50378085729   |\n",
      "| 1321 |  2.77516804913   |\n",
      "| 1322 |  4.25935484957   |\n",
      "| 1324 |  2.39590864578   |\n",
      "| 1326 |  3.17702122862   |\n",
      "| 1327 |  3.51191473975   |\n",
      "| 1328 |  1.88820217753   |\n",
      "| 1329 |  3.59448302481   |\n",
      "| 1330 |  2.34561356887   |\n",
      "| 1332 |  3.32725687476   |\n",
      "| 1333 |  3.57364969651   |\n",
      "| 1334 |  0.548450522444  |\n",
      "| 1335 |  2.63510895098   |\n",
      "| 1336 |  2.07555711751   |\n",
      "| 1339 |  3.00640286506   |\n",
      "| 1344 |  0.874136792633  |\n",
      "| 1346 |  3.69654430149   |\n",
      "| 1350 |  1.95231144639   |\n",
      "| 1351 |  2.69990607841   |\n",
      "| 1352 |  2.41699524889   |\n",
      "| 1354 |  1.68008247482   |\n",
      "| 1356 |  3.05547613096   |\n",
      "| 1358 |  3.90975707757   |\n",
      "| 1359 |  4.32433208192   |\n",
      "| 1361 |  5.16301935939   |\n",
      "| 1363 |  3.04051950303   |\n",
      "| 1364 |  4.84489062102   |\n",
      "| 1366 |  2.77832118106   |\n",
      "| 1368 |  2.18884736967   |\n",
      "| 1370 |  1.14598147542   |\n",
      "| 1371 |  2.04845100065   |\n",
      "| 1374 |  3.09506548405   |\n",
      "| 1375 |  3.55409590271   |\n",
      "| 1376 |  2.39049359878   |\n",
      "| 1377 |  3.18902418778   |\n",
      "| 1379 |  2.76845418986   |\n",
      "| 1380 |  3.20576182962   |\n",
      "| 1382 |  2.11549726108   |\n",
      "| 1384 |   3.8461643797   |\n",
      "| 1387 |  4.11175720938   |\n",
      "| 1388 |  1.33330366562   |\n",
      "| 1393 |  4.29105381954   |\n",
      "| 1395 |  2.52036064264   |\n",
      "| 1396 |  3.37485381952   |\n",
      "| 1398 |  3.92239415516   |\n",
      "| 1399 |  3.18556787661   |\n",
      "| 1401 |  1.37088505179   |\n",
      "| 1403 |  1.60687019233   |\n",
      "| 1404 |  1.91932278456   |\n",
      "| 1405 |  3.04882587871   |\n",
      "| 1408 |  2.85570134096   |\n",
      "| 1409 |  2.91925433515   |\n",
      "| 1411 |  1.75398957517   |\n",
      "| 1413 |  2.77072286492   |\n",
      "| 1414 |  1.95390525549   |\n",
      "| 1415 |  3.22096770315   |\n",
      "| 1417 |  3.32213094585   |\n",
      "| 1419 |  2.41307828321   |\n",
      "| 1421 |  3.80695796505   |\n",
      "| 1423 |  3.52655678961   |\n",
      "| 1427 |  4.76191156478   |\n",
      "| 1429 |  3.64059424116   |\n",
      "| 1430 |  2.61735052051   |\n",
      "| 1433 |   3.6354858915   |\n",
      "| 1434 |  2.27235428325   |\n",
      "| 1436 |  2.45116734077   |\n",
      "| 1438 |  2.68793795782   |\n",
      "| 1441 |  1.99037766881   |\n",
      "| 1443 |  2.15492212667   |\n",
      "| 1447 |  3.41002632538   |\n",
      "| 1451 |  3.24999720834   |\n",
      "| 1457 |   3.4246729421   |\n",
      "| 1459 |  2.75923998649   |\n",
      "| 1460 |  3.10311808097   |\n",
      "| 1462 |  1.19868666454   |\n",
      "| 1464 |  3.12918509198   |\n",
      "| 1466 |  1.56731287781   |\n",
      "| 1467 |  2.56166492507   |\n",
      "| 1469 |  2.29816924216   |\n",
      "| 1472 |  2.75624195405   |\n",
      "| 1475 |   3.2591528467   |\n",
      "| 1476 |  2.83410358646   |\n",
      "| 1479 |  3.58860590908   |\n",
      "| 1480 |  2.49623567589   |\n",
      "| 1483 |  2.32460843577   |\n",
      "| 1484 |  3.08915725528   |\n",
      "| 1485 |  3.09294310909   |\n",
      "| 1488 |  2.15789238061   |\n",
      "| 1490 |  2.70403533621   |\n",
      "| 1492 |  2.60999820161   |\n",
      "| 1493 |  2.95752421098   |\n",
      "| 1495 |  2.71520793899   |\n",
      "| 1496 |   3.2724163181   |\n",
      "| 1499 |  0.659432729719  |\n",
      "| 1502 |  0.980627601354  |\n",
      "| 1503 |  2.98136204452   |\n",
      "| 1505 |  2.32936000723   |\n",
      "| 1509 |  2.59257276318   |\n",
      "| 1510 |  2.50955627042   |\n",
      "| 1511 |  3.10072893403   |\n",
      "| 1512 |  1.70430365641   |\n",
      "| 1513 |  2.91029867454   |\n",
      "| 1515 |  1.11585693019   |\n",
      "| 1517 |  2.52899996661   |\n",
      "| 1520 |  1.27870624041   |\n",
      "| 1522 |  2.90956287031   |\n",
      "| 1523 |  3.58267554571   |\n",
      "| 1524 |  2.80901351025   |\n",
      "| 1526 |  3.24134057472   |\n",
      "| 1527 |  4.43639466722   |\n",
      "| 1528 |   1.4994841689   |\n",
      "| 1529 |  2.31304515373   |\n",
      "| 1533 |  2.78853762771   |\n",
      "| 1536 |  3.85789627141   |\n",
      "| 1537 |  3.09354703255   |\n",
      "| 1540 |  2.81659637453   |\n",
      "| 1541 |  2.54008375244   |\n",
      "| 1543 |  3.75650133393   |\n",
      "| 1544 |  3.08717977149   |\n",
      "| 1546 |  3.33165528334   |\n",
      "| 1548 |   3.4877942575   |\n",
      "| 1549 |  2.94338125574   |\n",
      "| 1550 |  2.92011085048   |\n",
      "| 1552 |  2.30270916827   |\n",
      "| 1553 |  2.85034220946   |\n",
      "| 1554 |  2.48839215642   |\n",
      "| 1555 |   3.0350964032   |\n",
      "| 1560 |  2.36628655672   |\n",
      "| 1564 |  2.83122709171   |\n",
      "| 1567 |  2.61951968945   |\n",
      "| 1570 |  1.87808325035   |\n",
      "| 1571 |  3.50947526838   |\n",
      "| 1573 |  2.14726265016   |\n",
      "| 1575 |  2.26136692516   |\n",
      "| 1576 |  4.84926700898   |\n",
      "| 1579 |  2.02530297216   |\n",
      "| 1580 |  2.82027759204   |\n",
      "| 1582 |  1.45455771456   |\n",
      "| 1583 |  4.33782907238   |\n",
      "| 1586 |  2.14220487912   |\n",
      "| 1587 |  4.00563851153   |\n",
      "| 1590 |  1.88049750581   |\n",
      "| 1591 |  2.23237086188   |\n",
      "| 1595 |  1.33096418815   |\n",
      "| 1597 |  4.25791727329   |\n",
      "| 1602 |  5.00548298401   |\n",
      "| 1603 |  3.72332738467   |\n",
      "| 1604 |  2.14263973076   |\n",
      "| 1605 |  3.50746478922   |\n",
      "| 1606 |  2.31262476028   |\n",
      "| 1607 |  2.31745288425   |\n",
      "| 1610 |  2.24808075002   |\n",
      "| 1611 |   1.9898303428   |\n",
      "| 1614 |  3.64820630551   |\n",
      "| 1617 |  3.70539816292   |\n",
      "| 1619 |  2.20582803221   |\n",
      "| 1623 |  1.75121905986   |\n",
      "| 1625 |  2.88249824379   |\n",
      "| 1626 |  3.41386129479   |\n",
      "| 1627 |  3.00702519945   |\n",
      "| 1632 |  3.91377463298   |\n",
      "| 1633 |  3.33910782919   |\n",
      "| 1634 |  2.76722731843   |\n",
      "| 1635 |  2.71446712979   |\n",
      "| 1636 |  3.86706547873   |\n",
      "| 1638 |  2.17787212749   |\n",
      "| 1639 |  2.34361607658   |\n",
      "| 1641 |  3.78642306421   |\n",
      "| 1644 |  4.44986454648   |\n",
      "| 1645 |  2.82285414873   |\n",
      "| 1649 |  0.842658983731  |\n",
      "| 1650 |  3.71533607173   |\n",
      "| 1651 |  2.11357715176   |\n",
      "| 1653 |  3.36859290075   |\n",
      "| 1654 |  2.45960595005   |\n",
      "| 1656 |  2.53293221738   |\n",
      "| 1657 |  1.34893331068   |\n",
      "| 1658 |   2.7634862166   |\n",
      "| 1659 |  3.88034229699   |\n",
      "| 1660 |  2.49465004048   |\n",
      "| 1661 |   3.4084904921   |\n",
      "| 1663 |  1.41343044799   |\n",
      "| 1664 |  2.06979577837   |\n",
      "| 1666 |  3.01742504043   |\n",
      "| 1667 |  3.07124244887   |\n",
      "| 1668 |  2.13327093367   |\n",
      "| 1669 |   2.4338559689   |\n",
      "| 1670 |  1.09713143122   |\n",
      "| 1671 |  2.02184620264   |\n",
      "| 1672 |  0.722420732664  |\n",
      "| 1678 |  4.31954703614   |\n",
      "| 1682 |  2.33266790237   |\n",
      "| 1683 |   1.9200847282   |\n",
      "| 1687 |  3.89671432107   |\n",
      "| 1689 |  2.32501544803   |\n",
      "| 1690 |  2.60503281263   |\n",
      "| 1692 |  1.71842381956   |\n",
      "| 1693 |  2.80780596774   |\n",
      "| 1696 |  2.23242715647   |\n",
      "| 1698 |  4.12836828566   |\n",
      "| 1700 |  3.49811635111   |\n",
      "| 1702 |  2.67918201914   |\n",
      "| 1704 |  1.20704641686   |\n",
      "| 1706 |  1.57059160154   |\n",
      "| 1707 |  2.94634062044   |\n",
      "| 1710 |  3.08041121022   |\n",
      "| 1711 |  1.35454862652   |\n",
      "| 1712 |  1.32758769658   |\n",
      "| 1717 |  1.74372695485   |\n",
      "| 1721 |  1.73711575297   |\n",
      "| 1722 |  1.25344348456   |\n",
      "| 1723 |  5.09420417897   |\n",
      "| 1725 |  2.49269356466   |\n",
      "| 1729 |  3.20396901262   |\n",
      "| 1730 |  3.62131841843   |\n",
      "| 1732 |  2.49972134753   |\n",
      "| 1733 |   1.270311474    |\n",
      "| 1735 |  3.48067699217   |\n",
      "| 1737 |  2.66260581995   |\n",
      "| 1739 |  2.54073847772   |\n",
      "| 1740 |   3.4351229877   |\n",
      "| 1741 |  1.64672067961   |\n",
      "| 1742 |  3.54270139411   |\n",
      "| 1744 |   4.1282454756   |\n",
      "| 1746 |  4.80359455227   |\n",
      "| 1752 |  1.95757411148   |\n",
      "| 1753 |  2.79667735645   |\n",
      "| 1754 |  3.84859579096   |\n",
      "| 1757 |   2.1884742988   |\n",
      "| 1759 |  2.29426146459   |\n",
      "| 1760 |  2.53582447106   |\n",
      "| 1761 |  4.18419573536   |\n",
      "| 1762 |  1.03864027844   |\n",
      "| 1763 |  2.61988561402   |\n",
      "| 1765 |  2.78369988463   |\n",
      "| 1766 |  3.43857738039   |\n",
      "| 1767 |   2.3378714991   |\n",
      "| 1769 |  2.44827816269   |\n",
      "| 1770 | -0.391859994926  |\n",
      "| 1771 |  3.28655855026   |\n",
      "| 1772 |  2.79300077576   |\n",
      "| 1774 |  2.37359929731   |\n",
      "| 1775 |  4.21286267254   |\n",
      "| 1778 |  4.18439560634   |\n",
      "| 1780 |  2.24288384041   |\n",
      "| 1781 |  3.31226325896   |\n",
      "| 1782 |  3.00891517258   |\n",
      "| 1785 |  0.172236670811  |\n",
      "| 1788 |  2.45247136095   |\n",
      "| 1790 |  2.99637344827   |\n",
      "| 1791 |  2.36781259384   |\n",
      "| 1792 |  3.30433697956   |\n",
      "| 1793 |  3.67320855141   |\n",
      "| 1795 |  4.33244934742   |\n",
      "| 1796 |   3.0503520337   |\n",
      "| 1798 |  1.61467308419   |\n",
      "| 1799 |  3.99856592904   |\n",
      "| 1801 |   3.7529184229   |\n",
      "| 1803 |  3.38329281453   |\n",
      "| 1805 |  2.20560825622   |\n",
      "| 1806 |  4.51831530008   |\n",
      "| 1811 |   3.3191295335   |\n",
      "| 1817 |  2.82909630136   |\n",
      "| 1820 |  2.53667027988   |\n",
      "| 1824 |   1.6762278722   |\n",
      "| 1827 |  4.11011106091   |\n",
      "| 1828 |  3.26258689177   |\n",
      "| 1829 |  3.59415353213   |\n",
      "| 1831 |  1.99111480122   |\n",
      "| 1832 |  2.07094644528   |\n",
      "| 1833 |  3.02739342088   |\n",
      "| 1834 |   3.1401647039   |\n",
      "| 1835 |  3.15399538741   |\n",
      "| 1839 |  2.72930001373   |\n",
      "| 1840 |  3.16926443228   |\n",
      "| 1842 |   3.7069068128   |\n",
      "| 1844 |  1.07080961218   |\n",
      "| 1845 |  3.35662142506   |\n",
      "| 1847 |  3.61633022963   |\n",
      "| 1848 |  2.97848944807   |\n",
      "| 1851 |  4.29279375981   |\n",
      "| 1852 |  2.28531401022   |\n",
      "| 1853 |   3.2867051573   |\n",
      "| 1857 |  3.25437131633   |\n",
      "| 1860 |   3.1110897086   |\n",
      "| 1861 |   4.6333811988   |\n",
      "| 1863 |  3.35209835624   |\n",
      "| 1864 |  3.19108235527   |\n",
      "| 1865 |  2.83721730986   |\n",
      "| 1867 |  1.92089247636   |\n",
      "| 1868 |  3.45479474022   |\n",
      "| 1870 |  2.90457033744   |\n",
      "| 1871 |  0.943330997862  |\n",
      "| 1873 |  2.62518305597   |\n",
      "| 1875 |  2.47665085824   |\n",
      "| 1876 |  3.70438750046   |\n",
      "| 1878 |  3.55645915345   |\n",
      "| 1881 |  2.02121369456   |\n",
      "| 1883 |  1.82738799986   |\n",
      "| 1884 |  3.74872692743   |\n",
      "| 1885 |  3.05514542457   |\n",
      "| 1887 |  3.24828580105   |\n",
      "| 1888 |  1.63422533382   |\n",
      "| 1889 |  2.20708476528   |\n",
      "| 1890 |  3.43326913552   |\n",
      "| 1892 |  2.01372250164   |\n",
      "| 1895 |  0.663094771675  |\n",
      "| 1896 |  1.75444296031   |\n",
      "| 1899 |  2.35696838835   |\n",
      "| 1901 |  2.05946217921   |\n",
      "| 1907 |  4.12668424978   |\n",
      "| 1908 |  3.96597098655   |\n",
      "| 1912 |  3.23806434373   |\n",
      "| 1913 |  3.45694809772   |\n",
      "| 1914 |   3.0858927889   |\n",
      "| 1920 |  2.41395562957   |\n",
      "| 1921 |  2.19828215941   |\n",
      "| 1925 |  2.39848463773   |\n",
      "| 1926 |  1.95245131131   |\n",
      "| 1927 |  2.75846547213   |\n",
      "| 1929 |  1.65925585292   |\n",
      "| 1930 |  3.17763718299   |\n",
      "| 1931 |  1.63740551006   |\n",
      "+------+------------------+\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos los datos de prueba\n",
    "test = read_csv('data/1.5/test.csv')\n",
    "test_id_col = test['Id'][:1000]\n",
    "X_test = test.ix[:, test.columns != 'Id'][:1000] #Quitamos la columna Id\n",
    "\n",
    "# Los preprocesamos y se los damos al SVR para que prediga el Hazard\n",
    "df_test = preprocess_data(X_test)\n",
    "predicted_values = svr.predict(df_test.values)\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "pt = PrettyTable()\n",
    "pt.add_column(\"Id\", test_id_col)\n",
    "pt.add_column(\"Predicted hazard\", predicted_values)\n",
    "print pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6. Cuantificación de la calidad de las predicciones\n",
    "\n",
    "Dependiendo el tipo de algoritmo utilizado existen diferentes formas de probar que tan confiable o que tan buen predictor de nuestros datos es el método usado.\n",
    "\n",
    "A continuación defina los siguientes scores y apliquelos de forma correspondiente a los métodos aplicados en los ejercicios de aprendizaje automático. \n",
    "\n",
    "* Regresión: \n",
    "    > *Mean Squared Error:* En estadística, el error cuadrático medio (ECM) de un estimador mide el promedio de los errores al cuadrado, es decir, la diferencia entre el estimador y lo que se estima. La diferencia se produce debido a la aleatoriedad o porque el estimador no tiene en cuenta la información que podría producir una estimación más precisa.\n",
    "    \n",
    "    > *r²:* En estadística, el coeficiente de determinación, es un estadístico usado en el contexto de un modelo estadístico cuyo principal propósito es predecir futuros resultados o probar una hipótesis. Determina la calidad del modelo para replicar los resultados, y la proporción de variación de los resultados que puede explicarse por el modelo.\n",
    "    \n",
    "* Clasificación: \n",
    "    > *F1 score*: El Valor-F (denominada también F-score o medida-F) en estadística es la medida de precisión que tiene un test. Se emplea en la determinación de un valor único ponderado de la precisión y la exhaustividad.\n",
    "    \n",
    "    > *Log loss*: Es una función que determina la pérdida logística (Seguir buscando....)\n",
    "    \n",
    "    > *Accuracy*: Es el grado de \"cercanía\" de las mediciones de una cantidad al verdadero valor de esa misma cantidad.\n",
    "    \n",
    "    > *Precision*: Es el grado en el que para repetidas mediciones de una cantidad bajo las mismas condiciones, se muestran los mismos resultados.\n",
    "    \n",
    "    > *Compute area under the curve*: La curva ROC se puede usar para generar estadísticos que resumen el rendimiento (o la efectividad, en su más amplio sentido) del clasificador.\n",
    "    \n",
    "    > El indicador más utilizado en muchos contextos es el área bajo la curva ROC o AUC. Este índice se puede interpretar como la probabilidad de que un clasificador ordenará o puntuará una instancia positiva elegida aleatoriamente más alta que una negativa.\n",
    "    \n",
    "* Clustering: \n",
    "    > *Rand index adjusted for chance*: El índice Rand o medida Rand (William M. Rand), es una medida de similitud entre dos clusters de datos. Una forma del índice Rand puede decirse ajustada por la probabilidad de agrupar elementos, en cuyo caso se denomina índice Rand ajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> MeanShift Clustering On Iris Dataset <\n",
      "Rand index adjusted for a chance: "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0dd7c4581915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Clustering score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"> MeanShift Clustering On Iris Dataset <\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Rand index adjusted for a chance: \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0madjusted_rand_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'iris' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "#Clustering score\n",
    "print \"> MeanShift Clustering On Iris Dataset <\"\n",
    "print \"Rand index adjusted for a chance: \" , adjusted_rand_score(iris.target, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
